Good Afternoon, I'm Gurparkash Singh, a fourth year undergrad in the Department of Computer Science and Engineering. This is my presentation for my B.Tech. Thesis project done under the guidance of Prof. Amit Sethi. The topic of my project is Robust Classifiers and I use Histopathology Datasets to test my models.

So the aim of Artificial Intelligence is to automate the tasks that have been traditionally done by humans. In the case of medical science, these tasks are performed by medical and health-care experts. Our goal is to use Machine Learning to automate these tasks. However, as we all know, Machine Learning, even though widely successful, requires large amounts of reliable data to perform well, and that is where the problem arises when dealing with Histopathology Datasets. These datasets come from various labs which use different conventions which cause these datasets to be noisy. Also often, the original Histopathology slides are too large to be able to examine independently and so, they are split into smaller images and are analyzed separately.

In this example, we see a large image from the Bach Dataset being split into smaller sections for a feasible analysis. The original image contains cancerous cells and is correctly labeled positive. However, when we split it into smaller images, we see that even though the sub-image might not have cancerous cells, it'll still be labeled positive, same as it's parent image. The lower image towards the right doesn't have any cancerous cells and is therefore mislabeled. Such noisy datasets make automatic medical image analysis a difficult task.

There is also some visual variance between the images, such as change in color. But classical techniques such as color normalization and color augmentation are often used to alleviate these issues up to some extent. However, there is no established technique to deal with issues stemming from noisy labels. As we know, Neural Networks are highly adept at function approximation. This means that they also overfit to any noise that is present in the dataset. Therefore, my goal is to build models and training techniques to train those models such that we get Robust Classifiers : Classifiers whose performance is not affected severely by the noise in the Training Data. The methods that I've come up with take inspiration from Curriculum Learning (invented by Yoshua Bengio et. al.) and Adversarial Autoencoders (invented by Ian Goodfellow et. al.).

So what can we learn from Curriculum Learning? The authors made an interesting observation that any model first fits to the non-noisy or easy samples. Only after it has done that, it starts overfitting to the noisy or difficult samples. Therefore, this serves as a motivation to come up with a method to train our models in an incremental way. First, we train on the easy samples. After that we add slightly more difficult points to our set and train on those. Similarly, we keep on adding more and more points to our set and stop whenever we want. Ideally, we would like to stop when only the noisy or the most difficult points are left. Of course, separating the noisy samples from clean samples is itself a herculean task. However what we can do is, come up with an approximation. That is exactly what the  authors did in the original Curriculum Learning paper. They used an upper bound on the loss values and updated the model parameters only for the points whose loss values were less than the upper bound. As the model trains and gets better and better, we can expect more and more points to come under that bound and be used in training. In this way, we can hope to train our model on roughly all the clean samples and not on the noisy samples. Why is this an approximate approach? Well, we know that sometimes Neural Networks work in mysterious ways and even some of the noisy samples may have low loss values and some clean samples might have high loss values. But overall, we can expect this kind of approach to work and our own model is similar in spirit to this model.

Next up, we have Adversarial Autoencoders. In their Vanilla form, Adversarial Autoencoders are used to fit the features or encodings to a prior probability distribution in an unsupervised way. An Adversarial Autoencoder consists of three parts, an encoder, a decoder, and a discriminator. The encoder consists of stacked convolutional layers and projects the input image to the feature space. The decoder takes these features which are output by the encoder and tries to reconstruct the original input image by passing the features through a network of transpose-convolutional layers. The goal of the discriminator is to distinguish between the features and the samples from prior distribution which are of the same dimensions as the features.

Here is a vanilla Adversarial Autoencoder that we just discussed. The whole system is trained in an adversarial way. The discriminator will try to reduce it's Cross Entropy Loss. The Decoder will try to reduce it's Reconstruction Loss. The Encoder will try to output the features such that the Decoder's loss is minimized whereas the discriminator's loss is maximized.

If the encoder is successfully able to fool the discriminator, it would mean that the discriminator is not able to separate the features from the prior. This means that our encoder is successful in fitting the features to the prior. At this point, our features should look something like this. As we can see, the green points are inseparable from the red points. The prior should be inseparable from the features. However, in it's vanilla form, the Adversarial Autoencoders are not helpful for us. We need features in which the different classes are also separable. The current model doesn't take the classes into account.

This is where we come with a small variation to the original version. Instead of training in a completely unsupervised way, we use the labels to fit the features to a multi-modal prior distribution with a different mode for each different class. The image below shows a bi-modal Gaussian distribution. If, somehow, we have features that look like this, we can easily train a classifier on top of these features.

Here is the updated model in which the discriminator also takes the class label as an input in addition to the features and prior samples. Rest of the network remains the same and is trained in exactly the same way as before. The prior here, as we can see, comes from a bi-modal distribution and if all goes well, we can expect our features to follow the same distribution.

So here's the hypothesis. The noisy samples in our dataset will not fit the prior properly as compared to the noise-free samples. The noise free samples will fit the distribution better. The intuition here is that the noisy samples be similar in appearance to the points in the other mode as compared to their own. So, we might be able to use the value of the PDF to separate the noisy samples from the noise free samples. Higher the value of the PDF, the more likely are the features to have come from a noise-free sample. 

This all seems fine. So, can we use these features to train a classifier? The sad answer is no. Once we have trained the Adversarial Autoencoder, we will have fixed the features. The main problem here, which separates it from the idea we discussed in curriculum learning is that the features are static. If we use a weighting of some kind, those weights will remain fixed and there will be no incremental learning as we discussed before. If we're not using any weighting, then the classifier will eventually overfit to the feature space similar to any normal neural network. Therefore, to have a model in which we can make use of the incremental learning approach, we need to co-train the classifier along with our Adversarial Autoencoder. So, to do this, we attach a classifier network to the Adversarial Autoencoder. The difference here is that now the encoder will try to output the features that also increase the accuracy of the classifier. Since the features are changing AS the classifier is training, the weighted approach should work in this case.

Here is our final model. The features from the encoder are used to calculate the weights which are used in the weighted cross entropy loss of our classifier. The features are also used by the classifier to predict the class of the features. The discriminator works in the same way as before. Next we discuss the different ways for weight calculation.

We can simply use the value of the PDF, but the range of its values would be hard to predict and it'll make the training a bit unstable. A better method is to normalize the weights for each mini-batch. This makes their values more manageable as they will sum up to 1. It also introduces regularization properties similar to batch normalization, since the values of weights for a data-point depends on other data-points in the mini-batch. In addition to that, we can keep a threshold to convert these weights into binary weights. I found this version to work the best.

Now, let's talk about the results. As expected, the features of the Adversarial Autoencoders are not robust enough and can't be used to train a classifier. The classifier easily overfits to the static features and eval set accuracy lies dismally in the seventies. Adding noise to the dataset worsens the performance even further.

To visualize the features, I use two dimensions at random from the multidimensional features and plot them. Of course, this doesn't show us the complete picture but it can still give us some idea of the distribution of the features. The yellow and green spots are the samples from our prior and the red and blue points correspond to the two different classes of our dataset. I have used the Breakhis Binary classification dataset. As we can see, the features from the two classes are not sufficiently separated. Instead, they lie in the middle of the two modes. This confirms our earlier speculation that we might need to co-train our classifier to generate more robust features.

Our model on the other hand performs significantly better than Adversarial Autoencoders. The results have been tabulated in the next slide and have been compared to the results of a convolutional network. The convolutional network uses batchnorm, dropout, as well as leaky-relu activation.

These are the features output by our model. Here we can see that the classes are much better separated. Again, we should keep in mind that we have only used two dimensions for visualization. In the original high dimension space, the features would be even better separated. The mean of our features for a class also coincides well with that of the prior distribution for the corresponding class. The smaller dots that lie away from the centers are the noisy samples and thus, will have lower PDF values as expected.

This is another visualization that corroborates the same.

This is the table that shows both the train-set and eval-set accuracies for our model as compared to conv-nets. As we can see, training accuracy of conv-nets remain mostly similar even with increasing noise values. This shows that they overfit to the noisy samples which is reflected in the decreasing eval-set accuracies. On the contrary, the eval-set accuracies of our model are undeterred by the noise and it's the training accuracy that decreases. It must be noted that the Evaluation set is free from noise. Thus we need our model to perform well on the eval-set even if the accuracy on the noisy training-set is low. The observations are presented as graphs in the next slides.

Again, this chart makes it clear that for conv-nets, the training accuracy remains the same whereas the eval-set accuracies plummet with increasing noise.

This chart shows an opposite trend for our model. Contrary to conv nets, our eval-set accuracies doesn't succumb to the increasing noise of the dataset and remain consistent.

The results show that our model is successfully able to perform well in the presence of noise. But is there any way to perform even better? Yes, there is a possibility. In our weighting technique, we either give low or zero weight to the noisy samples and are not using them to full effect. In a paper by Eric Arazo et. al., they have shown a way to make use of these noisy samples to build robust models. The basic idea is quite similar to curriculum learning. They noted that since initially, the network learns the noise-free samples before overfitting to the noise, we can use the prediction of the network itself to separate the noisy and the noise-free samples. To do this, they fit a binary mixture model to the loss values and use the weights in what they call a bootstrap loss function. I believe that our weighting method might be better than their method of fitting a model to 1 dimensional data. However, their bootstrap loss should work better than the weighted cross entropy loss that we have used. Currently, I'm trying to incorporate our own weighting method, to their bootstrap loss function. Since, in this method, the noisy samples also contribute to the learning procedure, we can expect to gain a further improvement to our model.

This is an image from their paper which shows the difference in loss values corresponding to clean and noisy samples. To this loss distribution they fit a classical beta-mixture-model to obtain the probability of the label of a sample being noisy from its loss value. We achieve the same goal by calculating the PDF from a multidimensional feature embedding. We can see that their noisy and clean samples already have a significant overlap which makes me believe that our approach might be better suited to the bootstrap loss function.

That is it from my side. Thank You!